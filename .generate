#!/usr/bin/env python

import json
import re
from six.moves import urllib
import ssl

import certifi
import sys
import hashlib
import HTMLParser
import time
import tldextract

class disposableHostGenerator():
    sources = {
        'list': [ 'https://gist.githubusercontent.com/adamloving/4401361/raw/',
                  'https://raw.githubusercontent.com/disposable/static-disposable-lists/master/mail-data-hosts-net.txt',
                  'https://raw.githubusercontent.com/disposable/static-disposable-lists/master/manual.txt',
                  'https://gist.githubusercontent.com/smeinecke/78b229031cc885a776c8b84c56e1c5ee/raw/0b2200109d68537c588066d05bc70b6bbe1d312d/generator_email_hosts',
                  'https://raw.githubusercontent.com/wesbos/burner-email-providers/master/emails.txt',
                  'https://www.stopforumspam.com/downloads/toxic_domains_whole.txt',
                  'https://raw.githubusercontent.com/martenson/disposable-email-domains/master/disposable_email_blocklist.conf',
                  'https://raw.githubusercontent.com/GeroldSetz/emailondeck.com-domains/master/emailondeck.com_domains_from_bdea.cc.txt',
                  'https://gist.githubusercontent.com/jamesonev/7e188c35fd5ca754c970e3a1caf045ef/raw/',
                  'https://raw.githubusercontent.com/willwhite/freemail/master/data/disposable.txt',
                  'https://raw.githubusercontent.com/stopforumspam/disposable_email_domains/master/blacklist.txt',
                  'https://raw.githubusercontent.com/daisy1754/jp-disposable-emails/master/list.txt',
                  'https://raw.githubusercontent.com/FGRibreau/mailchecker/master/list.txt',
                  'https://raw.githubusercontent.com/7c/fakefilter/main/txt/data.txt',
                  'https://raw.githubusercontent.com/mmilitzer/disposable/master/mailcom_highrisk.txt' ],
        'file': [ 'blacklist.txt' ],
        'json': [ 'https://api.mailpoof.com/domains', 'https://raw.githubusercontent.com/ivolo/disposable-email-domains/master/index.json',
                  'https://api.internal.temp-mail.io/api/v2/domains', 'https://inboxes.com/api/v2/domain' ],
        'sha1': [ 'https://raw.githubusercontent.com/GeroldSetz/Mailinator-Domains/master/mailinator_domains_from_bdea.cc.txt' ],
        'html': [ 'https://www.guerrillamail.com/en/',
                  'https://10minutemail.com/session/address', 'https://www.trash-mail.com/inbox/',
                  'https://correotemporal.org', 'https://fakemailgenerator.net',  
                  'https://www.temp-mails.com', 'https://lortemail.dk', 'https://yopmail.com/domain?d=all',
                  'https://tempmail.plus/en/' ],
        'discard.email': [ 'https://tempr.email/about-getDomains=55bea3fee498cb80f4d3060b738a5936.htm' ],
        'option-select-box': [ ],
        'whitelist': [ 'https://raw.githubusercontent.com/disposable/disposable/master/whitelist.txt' ],
        'whitelist_file': [ 'whitelist.txt' ],
        'custom': [ 'tempmailo' ]
    }

    scrape_sources = [ 'https://emailfake.com', 'https://mail-temp.com' ]

    domain_regex = re.compile(r'^[a-z\d-]{,63}(\.[a-z-]{,63})+$')
    domain_search_regex = re.compile(r'["\'\s>]([a-z\d\.-]{1,63}\.[a-z\-]{2,63})["\'\s<]')
    sha1_regex = re.compile(r'^[a-fA-F0-9]{40}')
    html_re = {
        'generic': re.compile("""<option[^>]*>@?([a-z0-9\-\.\&#;\d+]+)\s*(\(PW\))?<\/option>""", re.I),
        'www.temp-mails.com': re.compile("""<option.+?value="([^"]+)">\d+\s*\@""", re.I),
        'tempr.email': re.compile("""<option\s+value[^>]*>@?([a-z0-9\-\.\&#;\d+]+)\s*(\(PW\))?<\/option>""", re.I),
        'fakemailgenerator.net': re.compile('<a.+?data-mailhost="@?([a-z0-9\.-]{1,128})"', re.I),
        'emailfake.com': re.compile("""change_dropdown_list[^"]+"[^>]+>@?([a-z0-9\.-]{1,128})""", re.I),
        'mail-temp.com': re.compile("""change_dropdown_list[^"]+"[^>]+>@?([a-z0-9\.-]{1,128})""", re.I),
        '10minutemail.com': re.compile(""".+?@?([a-z0-9\.-]{1,128})""", re.I),
        'tempmailo.com': [re.compile("""servers = \[([^\]]+)\]""", re.I), re.compile("""'([^']+)'""")],
        'tempmail.plus': re.compile("""<button type="button" class="dropdown-item">([^<]+)</button>""", re.I),
        'luxusmail.org': re.compile('<a.+?domain-selector"[^>]+>@([a-z0-9\.-]{1,128})', re.I),
        'yopmail.com': re.compile(r'@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', re.I),
        'correotemporal.org': domain_search_regex
    }

    retry_errors_re = re.compile("""(The read operation timed out|urlopen error timed out)""", re.I)

    def __init__(self, options = None, out_file = None):
        self.no_mx = {}
        self.domains = {}
        self.sha1 = {}
        self.old_domains = {}
        self.old_sha1 = {}
        self.legacy_domains = {}
        self.source_map = {}
        self.skip = []
        self.scrape = []

        if not options:
            options = {}

        self.options = options
        self.supported_formats = list(self.sources.keys())
        self.out_file = 'domains' if out_file is None else out_file

        self.scrape_count = 0

    def verbosePrint(self, msg):
        if self.options.get('verbose'):
            print(msg)

    def fetch_url(self, url, headers = {}, timeout=3, raw=False, retry=None):
        retry = 0 if retry is None else retry
        if retry > 5:
            self.verbosePrint("Retry {0} for {1}".format(retry, url))

        headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:87.0) Gecko/20100101 Firefox/87.0')

        try:

            req = urllib.request.Request(
                url,
                data=None,
                headers=headers
            )

            context = ssl._create_unverified_context() 
            res = urllib.request.urlopen(req, timeout=timeout, context=context)
            if raw:
                return res
            return res.read() or ''
        except Exception as err:
            if self.retry_errors_re.search(str(err)) and retry < 15:
                time.sleep(1)
                return self.fetch_url(url, headers, timeout, raw, retry+1)

            self.verbosePrint('WRN Fetching URL {0} failed, see error: {1}'.format(url, err))
            return

    def process_source(self, url, fmt, encoding='utf-8', timeout=3, retry=None):
        if (fmt not in self.supported_formats) or (retry > 10):
            return

        retry = 0 if retry is None else retry
        if retry > 5:
            self.verbosePrint("Retry {0} for {1}".format(retry, url))

        lines = []
        data = ''
        if fmt in ('file', 'whitelist_file'):
            try:
                with open(url, 'rb') as f:
                    lines = [line.strip().decode('utf8') for line in f]
            except IOError:
                return

        elif fmt == 'custom':
            lines = getattr(self, "process_%s" % url)()
            if lines is None:
                return

        else:
            headers = {}
            if fmt == 'json':
                headers['Accept'] = 'application/json, text/javascript, */*; q=0.01'
                headers['X-Requested-With'] = 'XMLHttpRequest'
            data = self.fetch_url(url, headers, timeout)
            if data is None:
                return

        if fmt in ('whitelist', 'list'):
            lines = [line.decode(encoding) for line in data.splitlines()]
        elif fmt == 'json':
            raw = ''
            try:
                raw = json.loads(data.decode(encoding))
            except Exception as e:
                if 'Unexpected UTF-8 BOM' in str(e):
                    raw = json.loads(data.decode('utf-8-sig'))

            if 'domains' in raw:
                raw = raw['domains']
            if 'email' in raw:
                s = re.search('^.+?@?([a-z0-9\.-]{1,128})$', raw['email'])
                if s:
                    raw = [ s.group(1) ]
            if 'mailbox' in raw:
                s = re.search('^.+?@?([a-z0-9\.-]{1,128})$', raw['mailbox'])
                if s:
                    raw = [ s.group(1) ]

            if not isinstance(raw, list):
                self.verbosePrint('WRN This URL does not contain a JSON array: {0}'.format(url))
                return
            lines = list(filter(lambda line: line and isinstance(line, basestring), raw))
        elif fmt == 'option-select-box':
            dom_re = re.compile("""<option value="@?[^"]+">@?([a-z0-9\-\.]+\.[a-z0-9\-\.]+)<\/option>""", re.I)
            lines = dom_re.findall(data.decode(encoding))
        elif fmt == 'html':
            dom_re = self.html_re['generic']
            for (re_domain, re_item) in self.html_re.items():
                if re_domain != 'generic' and re_domain in url:
                    dom_re = re_item
                    break

            raw = data.decode(encoding)

            if type(dom_re) is list:
                for cl_re in dom_re[:-1]:
                    raw = '\n'.join(cl_re.findall(raw))
                dom_re = dom_re[-1]

            opts = dom_re.findall(raw)
            h = HTMLParser.HTMLParser()
            lines = list(map(lambda opt: h.unescape(opt[0]) if type(opt) is tuple else opt, opts))
        elif fmt == 'sha1':
            lines = data.splitlines()
            lines = [line.decode('ascii').lower() for line in lines]
            for sha1_str in lines:
                if not sha1_str or not self.sha1_regex.match(sha1_str):
                    continue

                self.sha1[sha1_str.lower()] = None
            return True

        lines_filtered = [line.lower().strip(' .,;@') for line in lines]
        lines_filtered = list(filter(lambda line: self.checkValidDomain(line), lines_filtered))

        if not lines_filtered:
            lines_filtered = self.domain_search_regex.findall(str(data))

        if fmt in ('whitelist', 'whitelist_file'):
            for host in lines:
                if not host in self.skip:
                    self.skip.append(host)
            return True

        if not lines_filtered:
            self.verbosePrint('WRN No results for this source: {0}'.format(url))
            return

        if url in self.scrape_sources:
            self.source_map[url] = self.scrape
        else:
            self.source_map[url] = lines_filtered
        need_scrape = False

        for host in lines_filtered:
            self.domains[host] = None
            try:
                self.sha1[hashlib.sha1(host.encode('idna')).hexdigest()] = None
            except:
                pass
            self.legacy_domains[host] = None

            if url in self.scrape_sources and not host in self.scrape:
                self.scrape.append(host)
                need_scrape = True
                self.scrape_count = 0

        if not need_scrape and url in self.scrape_sources and self.scrape_count < 2 and retry > 0:
            self.scrape_count += 1
            need_scrape = True

        if need_scrape:
            time.sleep(8)
            return self.process_source(url, fmt, encoding, timeout, retry+1)

        self.scrape_count = 0
        return True

    def process_tempmailo(self):
        res = self.fetch_url('https://tempmailo.com/', raw=True)
        if res is None:
            return

        cookies = {}
        for ky, vl in res.info().items():
            if ky != 'set-cookie':
                continue
            vl = vl[vl.index(".AspNetCore"):]
            (ck_name, ck_data) = vl.split('=', 1)
            if ck_name.startswith('__'):
                continue
            (ck_value, ck_rest) = ck_data.split(';', 1)
            cookies[ck_name] = ck_value

        body = res.read().decode('utf8')

        f = re.search('name="__RequestVerificationToken".+?value="([^"]+)"', body)
        if not f:
            self.verbosePrint('WRN Failed to fetch __RequestVerificationToken')
            return

        cookie = []
        for ky, vl in cookies.items():
            cookie.append(ky + '=' + vl)

        headers = {
            'requestverificationtoken': f.group(1),
            'accept': 'application/json, text/plain, */*',
            'x-requested-with': 'XMLHttpRequest',
            'referer': 'https://tempmailo.com/',
            'cookie': '; '.join(cookie)
        }

        retry = 0
        lines = []
        while retry < 15:
            data = self.fetch_url('https://tempmailo.com/changemail', headers=headers)
            if not data:
                return data

            for line in data.splitlines():
                (name, domain) = line.decode('utf8').split('@', 1)
                lines.append(domain)

            retry = retry + 1
            time.sleep(10)

        return lines

    """ read and compare to current (old) domains file
    """
    def readFiles(self):
        self.old_domains = {}
        try:
            with open(self.out_file + '.txt') as f:
                for line in f:
                    self.old_domains[line.strip()] = None
        except IOError:
            pass

        self.old_sha1 = {}
        try:
            with open(self.out_file + '_sha1.txt') as f:
                for line in f:
                    self.old_sha1[line.strip()] = None
        except IOError:
            pass

        self.legacy_domains = {}
        try:
            with open(self.out_file + '_legacy.txt') as f:
                for line in f:
                    self.legacy_domains[line.strip()] = None
        except IOError:
            pass

    """ check if given host is not a TLD and a valid domainname
    """
    def checkValidDomain(self, host):
        try:
            if not self.domain_regex.match(host):
                return False

            t = tldextract.extract(host)
            return (t.domain != '' and t.suffix != '')
        except Exception:
            pass

        return False

    """ merge all lists
    """
    def generate(self):
        # build domains dict
        for fmt in self.supported_formats:
            for src in self.sources[fmt]:
                if self.options.get('src_filter') is not None and \
                   not src == self.options.get('src_filter'):
                    continue

                try:
                    self.process_source(src, fmt)
                except Exception as err:
                    self.verbosePrint((src, fmt, err))
                    raise err

        # add custom whitelist
        for domain in self.skip:
            self.domains.pop(domain, None)
            self.sha1.pop(hashlib.sha1(domain.encode('idna')).hexdigest(), None)

        if self.options.get('verbose'):
            if not self.old_domains:
                self.readFiles()

            added = list(
                filter(lambda domain: domain not in self.old_domains, self.domains.keys()))
            removed = list(
                filter(lambda domain: domain not in self.domains, self.old_domains.keys()))

            added_sha1 = list(
                filter(lambda sha_str: sha_str not in self.old_sha1, self.sha1.keys()))
            removed_sha1 = list(
                filter(lambda sha_str: sha_str not in self.sha1, self.old_sha1.keys()))

            self.verbosePrint('Fetched {0} domains and {1} hashes'.format(len(self.domains), len(self.sha1)))
            self.verbosePrint(' - {0} domain(s) added'.format(len(added)))
            self.verbosePrint(' - {0} domain(s) removed'.format(len(removed)))
            self.verbosePrint(' - {0} hash(es) added'.format(len(added_sha1)))
            self.verbosePrint(' - {0} hash(es) removed'.format(len(removed_sha1)))
            # stop if nothing has changed
            if len(added) == len(removed) == len(added_sha1) == len(removed_sha1) == 0:
                return False

            if self.options.get('src_filter'):
                self.verbosePrint(self.domains.keys())

        return True

    def writeToFile(self):
        # write new list to file(s)
        domains = list(self.domains.keys())
        domains.sort()
        with open(self.out_file + '.txt', 'w') as ff:
            ff.write('\n'.join(domains))

        with open(self.out_file + '.json', 'w') as ff:
            ff.write(json.dumps(domains))

        if self.options.get('source_map'):
            with open(self.out_file + '_source_map.txt', 'w') as ff:
                for (src_url, source_map_domains) in sorted(self.source_map.items()):
                    source_map_domains.sort()
                    ff.write(src_url + ':' + ('\n%s:' % src_url).join(source_map_domains) + "\n")

        if self.no_mx:
            domains_with_mx = self.domains
            for domain in self.no_mx:
                domains_with_mx.pop(domain, None)

            domains = list(domains_with_mx.keys())
            domains.sort()
            with open(self.out_file + '_mx.txt', 'w') as ff:
                ff.write('\n'.join(domains))

            with open(self.out_file + '_mx.json', 'w') as ff:
                ff.write(json.dumps(domains))

        # write new hash list to file(s)
        domains_sha1 = list(self.sha1.keys())
        domains_sha1.sort()
        with open(self.out_file + '_sha1.txt', 'w') as ff:
            ff.write('\n'.join(domains_sha1))

        with open(self.out_file + '_sha1.json', 'w') as ff:
            ff.write(json.dumps(domains_sha1))

if __name__ == '__main__':
    exit_status = 1
    options = {
        'source_map': True if '--source-map' in sys.argv else False,
        'src_filter': None,
        'verbose': False if '--quiet' in sys.argv else True
    }

    if '--src' in sys.argv:
        options['src_filter'] = sys.argv[sys.argv.index('--src')+1]

    dhg = disposableHostGenerator(options)
    if dhg.generate() or options.get('src_filter') is not None:
        exit_status = 0
        dhg.writeToFile()
    sys.exit(exit_status)
